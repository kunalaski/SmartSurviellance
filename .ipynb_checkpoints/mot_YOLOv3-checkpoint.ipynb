{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "from motrackers.detectors import YOLOv3\n",
    "from motrackers import CentroidTracker, CentroidKF_Tracker, SORT, IOUTracker\n",
    "#from motrackers.utils import draw_tracks\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_FILE = \"test.mp4\"\n",
    "WEIGHTS_PATH = './examples/pretrained_models/yolo_weights/yolov4.weights'\n",
    "CONFIG_FILE_PATH = './examples/pretrained_models/yolo_weights/yolov4.cfg'\n",
    "LABELS_PATH = \"./examples/pretrained_models/yolo_weights/coco_names.json\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "\n",
    "MAX_SEQ_LENGTH = 2000\n",
    "NUM_FEATURES = 2048\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.2\n",
    "DRAW_BOUNDING_BOXES = True\n",
    "USE_GPU = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tracker = CentroidKF_Tracker(max_lost=0, tracker_output_format='mot_challenge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv3(\n",
    "    weights_path=WEIGHTS_PATH,\n",
    "    configfile_path=CONFIG_FILE_PATH,\n",
    "    labels_path=LABELS_PATH,\n",
    "    confidence_threshold=CONFIDENCE_THRESHOLD,\n",
    "    nms_threshold=NMS_THRESHOLD,\n",
    "    draw_bboxes=DRAW_BOUNDING_BOXES,\n",
    "    use_gpu=USE_GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocrop(image, threshold=0):\n",
    "    \"\"\"Crops any edges below or equal to threshold\n",
    "\n",
    "    Crops blank image to 1x1.\n",
    "\n",
    "    Returns cropped image.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(image.shape) == 3:\n",
    "        flatImage = np.max(image, 2)\n",
    "    else:\n",
    "        flatImage = image\n",
    "    assert len(flatImage.shape) == 2\n",
    "\n",
    "    rows = np.where(np.max(flatImage, 0) > threshold)[0]\n",
    "    if rows.size:\n",
    "        cols = np.where(np.max(flatImage, 1) > threshold)[0]\n",
    "        image = image[cols[0]: cols[-1] + 1, rows[0]: rows[-1] + 1]\n",
    "    else:\n",
    "        image = image[:1, :1]\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main(video_path, model, tracker):\n",
    "    frames = []\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    colorTracker = {}\n",
    "    trk_dict={}\n",
    "    count = 0\n",
    "    prev_frame_time = 0\n",
    "    new_frame_time = 0\n",
    "    try:\n",
    "        while True:\n",
    "            ok, image = cap.read()\n",
    "\n",
    "            if not ok:\n",
    "                print(\"Cannot read the video feed.\")\n",
    "                break\n",
    "            \n",
    "            image = cv.resize(image, (700, 500))\n",
    "            booli, output = model.detect(image)\n",
    "            if(booli != False):\n",
    "                bboxes, confidences, class_ids = output\n",
    "                tracks = tracker.update(bboxes, confidences, class_ids)\n",
    "                #print(class_ids)\n",
    "    #             print(len(tracks))\n",
    "                image, colorTracker = model.draw_bboxes(image.copy(), bboxes, confidences, class_ids, tracks, colorTracker)\n",
    "    #             print(updated_image)\n",
    "                \n",
    "                #frame = crop_center_square(updated_image)\n",
    "#             gray = cv.cvtColor(image,cv.COLOR_BGR2GRAY)\n",
    "#             _,thresh = cv.threshold(gray,1,255,cv.THRESH_BINARY)\n",
    "#             contours,hierarchy = cv.findContours(thresh,cv.RETR_EXTERNAL,cv.CHAIN_APPROX_SIMPLE)\n",
    "#             cnt = contours[0]\n",
    "#             x,y,w,h = cv.boundingRect(cnt)\n",
    "#             frame = image[y:y+h,x:x+w]\n",
    "            #print(image.shape)\n",
    "            image = autocrop(image)\n",
    "            frame = cv.resize(image, (224, 224))\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "            frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
    "           # frame = frame[20:210, 10:210]\n",
    "            frames.append(frame)\n",
    "            count += 1\n",
    "            #print(count)\n",
    "            #print(type(frames))\n",
    "            new_frame_time = time.time()\n",
    "            fps = 1/(new_frame_time-prev_frame_time)\n",
    "            prev_frame_time = new_frame_time\n",
    "            #print(f'{count}-{fps}')\n",
    "            cv.imshow(\"image\", frame)\n",
    "            if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "    return np.array(frames)\n",
    "#main('D:\\kunal\\Project\\smart surveillance\\ModelCode\\TrainTestData\\Anomaly-Videos-Part-1\\Arrest\\Arrest029_x264.mp4', model, tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(path):\n",
    "    try:\n",
    "        os.stat(path)\n",
    "    except:\n",
    "        os.mkdir(path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dirs():\n",
    "    create_dir('E:/dataset/')\n",
    "    create_dir('E:/dataset/Train/')\n",
    "    create_dir('E:/dataset/Test')\n",
    "    create_dir('E:/dataset/Train/Anomaly/')\n",
    "    create_dir('E:/dataset/Test/Anomaly/')\n",
    "    create_dir('E:/dataset/Train/Normal/')\n",
    "    create_dir('E:/dataset/Test/Normal/')\n",
    "initialize_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(start_path = 'data/'):\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAno = pd.read_csv(\"AnomalyTrain.csv\")\n",
    "testAno = pd.read_csv(\"AnomalyTest.csv\")\n",
    "testNor = pd.read_csv(\"NormalTrain.csv\")\n",
    "trainNor = pd.read_csv(\"NormalTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainAno.at[78, 'path']\n",
    "for i in range(78):\n",
    "    trainAno.at[i, 'progress'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\kunal\\Project\\smart surveillance\\ModelCode\\TrainTestData\\Anomaly-Videos-Part-1\\Arrest\\Arrest039_x264.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19564/43914188.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mvid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainAno\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mframes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19564/523081307.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(video_path, model, tracker)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mbooli\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooli\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0mbboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\kunal\\Project\\smart surveillance\\SmartSurviellance\\motrackers\\detectors\\yolo.py\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mbboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfidences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\kunal\\Project\\smart surveillance\\SmartSurviellance\\motrackers\\detectors\\yolo.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mblob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblobFromImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mswapRB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_names\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# detect objects using object detection model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in trainAno.index:\n",
    "    if trainAno.at[i, 'progress'] == -1:\n",
    "        vid = trainAno.at[i, 'path']\n",
    "        print(vid)\n",
    "        frames = main(vid, model, tracker)\n",
    "        count = 0\n",
    "        for j in frames:\n",
    "            cv.imwrite('E:/dataset/Train/Anomaly/' + vid.split('\\\\')[-1].split('.')[0]+f'_frame{count}.jpg', j)\n",
    "            count+=1\n",
    "        trainAno.at[i, 'progress'] = 1\n",
    "        trainAno.at[i, 'count'] = count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAno.to_csv('AnomalyTrain.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAno1 = prepare_all_videos(trainAno, 'dataset/Train/Anomaly/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"count.txt\", 'r') as fp:\n",
    "    x = len(fp.readlines())\n",
    "    print('Total lines:', x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testAno1 = prepare_all_videos(testAno, 'dataset/Test/Anomaly/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNor1 = prepare_all_videos(testNor, 'dataset/Test/Normal/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2 as cv\n",
    "frames = []\n",
    "colorTracker = {}\n",
    "image = cv.imread(\"street2.jpg\")\n",
    "image = cv.resize(image, (700, 500))\n",
    "booli, output = model.detect(image)\n",
    "if(booli == False):\n",
    "    frame = cv.resize(output, (224, 224))\n",
    "    #print(f'false:{count}')\n",
    "else:\n",
    "    bboxes, confidences, class_ids = output\n",
    "    tracks = tracker.update(bboxes, confidences, class_ids)\n",
    "    #print(class_ids)\n",
    "#             print(len(tracks))\n",
    "    frame, colorTracker = model.draw_bboxes(image.copy(), bboxes, confidences, class_ids, tracks, colorTracker)\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_RGB2BGR)\n",
    "    fig = plt.figure(figsize=(12,7))\n",
    "    plt.imshow(frame)\n",
    "# frame = frame[20:210, 10:210]\n",
    "#print(count)\n",
    "#print(type(frames))\n",
    "#print(f'{count}-{fps}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('obj2.jpg', bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (224,224)\n",
    "POSITIVES_PATH_TRAIN = 'data/Train/Class1/'\n",
    "NEGATIVES_PATH_TRAIN = 'data/Train/Class2/'\n",
    "\n",
    "POSITIVES_PATH_VALID = 'data/Val/Class1/'\n",
    "NEGATIVES_PATH_VALID = 'data/Val/Class2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg_conv.summary()\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(vgg_conv, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add the vgg convolutional base model\n",
    "    model.add(vgg_conv)\n",
    "\n",
    "    # Add new layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(256, activation = 'relu'))\n",
    "    model.add(layers.Dense(2, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "build_feature_extractor().summary()\n",
    "tf.keras.utils.plot_model(build_feature_extractor(), to_file='model2.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchsize = 64\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "train_generator = train_datagen.flow_from_directory('data/Train/', class_mode='categorical', batch_size=train_batchsize, target_size = SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    "val_generator = val_datagen.flow_from_directory('data/Val/', class_mode='categorical', batch_size=train_batchsize, target_size = SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "model = build_feat_extractor()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# Train the model\n",
    "model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
    "      validation_data = val_generator,\n",
    "      validation_steps = val_generator.samples/val_generator.batch_size,\n",
    "      epochs=10,\n",
    "      verbose=2)\n",
    " \n",
    "# Save the trained model to disk\n",
    "model.save('weights/Feature_Extractor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.layers as L\n",
    "\n",
    "inp = model.input\n",
    "out = model.layers[-4].output\n",
    "feat_extractor = Model(inputs = [inp], outputs = [out])\n",
    "feat_extractor.summary()\n",
    "\n",
    "feat_extractor.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "LOOK_BACK = 4\n",
    "\n",
    "def data_to_lstm_format(POSITIVES_PATH, NEGATIVES_PATH, look_back = 4):\n",
    "    data = np.array([])\n",
    "    labels = np.array([])\n",
    "    numbers = []\n",
    "    # POSITIVE LABELS\n",
    "    for value in os.listdir(POSITIVES_PATH):\n",
    "        numbers.append(int(re.findall(r'\\d+', value.split('_')[2])[0]))\n",
    "\n",
    "    # filter by video\n",
    "    for numb in np.unique(numbers):\n",
    "        frames = []\n",
    "        # append image name\n",
    "        for value in os.listdir(POSITIVES_PATH):\n",
    "            if int(re.findall(r'\\d+', value.split('_')[2])[0]) == numb:\n",
    "                frames.append(value)\n",
    "        # sort image frame by frame number\n",
    "        frames = sorted(frames, key = lambda x: int(re.findall(r'\\d+', x.split('_')[-1].split('.')[0])[0]))\n",
    "        image_data = np.zeros((len(frames), 1024))\n",
    "\n",
    "        # get feature vector from vgg16 for each frame and stack\n",
    "        for index, image in enumerate(frames):\n",
    "            img = cv2.imread(POSITIVES_PATH + image)\n",
    "            vect = feat_extractor.predict(img.reshape(1,224,224,3))\n",
    "            image_data[index,:] = vect\n",
    "\n",
    "        # for each frame get tensor with lookbacks\n",
    "        stacked_data = np.zeros((len(frames), look_back, 1024))\n",
    "        for index in range(len(frames)):\n",
    "            labels = np.append(labels, [1])\n",
    "            stacked_data[index, 0, :] = image_data[index]\n",
    "            for lb in range(1, look_back):\n",
    "                if index - lb >= 0:\n",
    "                    stacked_data[index, lb, :] = image_data[index - lb]\n",
    "                else:\n",
    "                    stacked_data[index, lb, :] = np.zeros(1024)\n",
    "\n",
    "        if data.shape[0] == 0:\n",
    "            data = stacked_data\n",
    "        else:\n",
    "            data = np.concatenate([data, stacked_data], axis = 0)\n",
    "\n",
    "\n",
    "\n",
    "    for value in os.listdir(NEGATIVES_PATH):\n",
    "        numbers.append(int(re.findall(r'\\d+', value.split('_')[2])[0]))\n",
    "\n",
    "    # filter by video\n",
    "    for numb in np.unique(numbers):\n",
    "        frames = []\n",
    "        # append image name\n",
    "        for value in os.listdir(NEGATIVES_PATH):\n",
    "            if int(re.findall(r'\\d+', value.split('_')[2])[0]) == numb:\n",
    "                frames.append(value)\n",
    "        # sort image frame by frame number\n",
    "        frames = sorted(frames, key = lambda x: int(re.findall(r'\\d+', x.split('_')[-1].split('.')[0])[0]))\n",
    "        image_data = np.zeros((len(frames), 1024))\n",
    "\n",
    "        # get feature vector from vgg16 for each frame and stack\n",
    "        for index, image in enumerate(frames):\n",
    "            img = cv2.imread(NEGATIVES_PATH + image)\n",
    "            vect = feat_extractor.predict(img.reshape(1,224,224,3))\n",
    "            image_data[index,:] = vect\n",
    "\n",
    "        # for each frame get tensor with lookbacks\n",
    "        stacked_data = np.zeros((len(frames), look_back, 1024))\n",
    "        for index in range(len(frames)):\n",
    "            labels = np.append(labels, [0])\n",
    "            stacked_data[index, 0, :] = image_data[index]\n",
    "            for lb in range(1, look_back):\n",
    "                if index - lb >= 0:\n",
    "                    stacked_data[index, lb, :] = image_data[index - lb]\n",
    "                else:\n",
    "                    stacked_data[index, lb, :] = np.zeros(1024)\n",
    "\n",
    "        if data.shape[0] == 0:\n",
    "            data = stacked_data\n",
    "        else:\n",
    "            data = np.concatenate([data, stacked_data], axis = 0)\n",
    "\n",
    "    # one hot labels\n",
    "    from keras.utils import to_categorical\n",
    "    labels = to_categorical(labels)\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data, tr_labels = data_to_lstm_format(POSITIVES_PATH_TRAIN, NEGATIVES_PATH_TRAIN, look_back=LOOK_BACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, val_labels = data_to_lstm_format(POSITIVES_PATH_VALID, NEGATIVES_PATH_VALID, look_back=LOOK_BACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dropout\n",
    "num_features = 1024\n",
    "\n",
    "def build_model():\n",
    "    inp = L.Input(shape = (4, num_features))\n",
    "    \n",
    "#     \"\"\" Use CuDNNLSTM if your machine supports CUDA\n",
    "#         Training time is significantly faster compared to LSTM \"\"\"\n",
    "    \n",
    "#     x = L.LSTM(64, return_sequences = True)(inp)\n",
    "#     #x = L.CuDNNLSTM(64, return_sequences = True)(inp)\n",
    "#     x = L.Dropout(0.2)(x)\n",
    "#     #x = L.LSTM(16)(x)\n",
    "#     x = L.Bidirectional(LSTM(16))(x)\n",
    "# #    x = L.CuDNNLSTM(16)(x)\n",
    "#     out = L.Dense(2, activation = 'softmax')(x)\n",
    "#     model = Model(inputs = [inp], outputs = [out])\n",
    "#     model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "#               metrics=['acc'])\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True), input_shape=(4, 1024)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(TimeDistributed(Dense(2, activation='softmax')))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bimod = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.utils.plot_model(bimod, to_file='model3.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "##https://www.tensorflow.org/tensorboard/get_started\n",
    "\n",
    "log_dir = \"data/_training_logs/rnn/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
    "\n",
    "model = build_model()\n",
    "history = model.fit(tr_data, tr_labels, \n",
    "                    validation_data = (val_data, val_labels),\n",
    "                    callbacks = [tensorboard_callback],\n",
    "                    verbose = 2, epochs = 20, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights/RNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap=cv2.VideoCapture('test.mp4')\n",
    "ret1,frame1= cap.read()\n",
    "gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "gray1 = cv2.GaussianBlur(gray1, (21, 21), 0)\n",
    "cv2.imshow('window',frame1)\n",
    "while(True):\n",
    "    ret2,frame2=cap.read()\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.GaussianBlur(gray2, (21, 21), 0)\n",
    "    \n",
    "    deltaframe=cv2.absdiff(gray1,gray2)\n",
    "    cv2.imshow('delta',deltaframe)\n",
    "    threshold = cv2.threshold(deltaframe, 25, 255, cv2.THRESH_BINARY)[1]\n",
    "    threshold = cv2.dilate(threshold,None)\n",
    "    cv2.imshow('threshold',threshold)\n",
    "    countour,heirarchy = cv2.findContours(threshold, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    for i in countour:\n",
    "        if cv2.contourArea(i) < 50:\n",
    "            continue\n",
    " \n",
    "        (x, y, w, h) = cv2.boundingRect(i)\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "    \n",
    "    cv2.imshow('window',frame2)\n",
    "    \n",
    "    if cv2.waitKey(20) == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manDown(video_path, model, tracker):\n",
    "    frames = []\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    try:\n",
    "        while True:\n",
    "            ok, image = cap.read()\n",
    "\n",
    "            if not ok:\n",
    "                print(\"Cannot read the video feed.\")\n",
    "                break\n",
    "\n",
    "            image = cv.resize(image, (700, 500))\n",
    "            booli, output = model.detect(image)\n",
    "            if(booli == False):\n",
    "                cv.imshow(\"image\", image)\n",
    "                frames.append(image)\n",
    "            else:\n",
    "                centroid_dict = dict() \n",
    "                objectId = 0\n",
    "                bboxes, confidences, class_ids = output\n",
    "                tracks = tracker.update(bboxes, confidences, class_ids)\n",
    "                \n",
    "                for bb, conf, cid, trk in zip(bboxes, confidences, class_ids, tracks):\n",
    "                    if (model.checkPerson):\n",
    "                        xmin, ymin, xmax, ymax = model.convertBack(float(bb[0]), float(bb[1]), float(bb[2]), float(bb[3]))\n",
    "                        centroid_dict[objectId] = (int(bb[0]), int(bb[1]), xmin, ymin, xmax, ymax)\n",
    "                \n",
    "                    #print(tracks)\n",
    "                    objId = tracks[0]\n",
    "                    fall_alert_list = [] \n",
    "                    red_line_list = []\n",
    "                    for id,p in centroid_dict.items():\n",
    "                        dx, dy = p[4] - p[2], p[5] - p[3]  \n",
    "                        difference = dy-dx\n",
    "                        if difference < 0:\n",
    "                            fall_alert_list.append(id)      \n",
    "\n",
    "#                     for idx, box in centroid_dict.items():  # dict (1(key):red(value), 2 blue)  idx - key  box - value\n",
    "#                         if idx in fall_alert_list:   # if id is in red zone list\n",
    "#                             cv.rectangle(image, (box[2], box[3]), (box[4], box[5]), (255, 0, 0), 2) # Create Red bounding boxes  #starting point, ending point size of 2\n",
    "#                         else:\n",
    "#                             cv.rectangle(image, (box[2], box[3]), (box[4], box[5]), (0, 255, 0), 2) \n",
    "\n",
    "                    if len(fall_alert_list)!=0:\n",
    "                        text = \"Fall Detected\"\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        text = \"Fall Not Detected\"\n",
    "                        alert_var = 0           # makes sure that alert is generated when there are 20 simultaeous frames of fall detection\n",
    "\n",
    "                    location = (10,25)\n",
    "                    if len(fall_alert_list)!=0:\n",
    "                        cv.putText(image, text, location, cv.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2, cv.LINE_AA)  # Display Text\n",
    "                    else:\n",
    "                        cv.putText(image, text, location, cv.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2, cv.LINE_AA)  # Display Text\n",
    "\n",
    "                    cv.imshow(\"image\", image)\n",
    "            if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv.destroyAllWindows()\n",
    "    #return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manDown('fall.mp4', model, tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U numpy==1.19.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
